{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## COMP30830 - Software Engineering\n",
    "### Project: DublinBikes  \n",
    "Group work by:  \n",
    "\n",
    "**Xi Jiang  \n",
    "Yuqian Shu  \n",
    "Yi Zhang**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Machine Learning prediction model\n",
    "In this notebook we develop a ML model to predict two key values that are functional to our web application:  \n",
    "  \n",
    "**number of available bikes for a selected station  \n",
    "number of available bike stands for a selected station**  \n",
    "  \n",
    "The model is trained on data fetched from an Amazon RDS database, where we store historical data referring to:\n",
    "  \n",
    "real-time information that Dublin Bikes makes available through their web API  \n",
    "Dublin weather information from OpenWeather, obtained by a web API  \n",
    "***N.B.:*** all database data are kept up-to-date."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Getting started\n",
    "First of all, we need to import Python packages that are required to our model, as well as fetching the data from the Amazon RDS database."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Import required packages\n",
    "A number of Python packages are required for our model in order to work, so we import them in our Notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle \n",
    "#import seaborn as sns\n",
    "#import matplotlib.pyplot as plt\n",
    "import pandas as pd \n",
    "#import numpy as np \n",
    "#import json\n",
    "import pymysql\n",
    "\n",
    "#from sklearn.linear_model import LinearRegression\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.model_selection import train_test_split\n",
    "#from sklearn.preprocessing import PolynomialFeatures\n",
    "#from sklearn.pipeline import make_pipeline\n",
    "#from patsy import dmatrices\n",
    "from sklearn import metrics\n",
    "#from sklearn.model_selection import cross_validate"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Connect to Amazon RDS database\n",
    "After having stored all the necessary credentials in specific variables, we connect to the database providing error-handling in case of connection issues."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Attempt connection to database\n",
    "# Print a statement on the screen to check whether the connection is working\n",
    "try:\n",
    "    con = pymysql.connect(host='dublinbike.cczltqdfsp1t.eu-west-1.rds.amazonaws.com', user='root', passwd='shuyuqian',db='dublin')\n",
    "    print('+=========================+')\n",
    "    print('|    TRYING TO CONNECT    |')\n",
    "    print('+=========================+')\n",
    "    print('|        SUCCESS!         |')\n",
    "    print('+=========================+')\n",
    "    \n",
    "# Exit if connection not working   \n",
    "except Exception as e:\n",
    "        sys.exit(e)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Data fetching\n",
    "Bikes information\n",
    "We store real-time data from Dublin Bikes in a relation named \"availability\" in the RDS database schema.  \n",
    "We store weather data from OpenWeather in a relation named \"weather\" in the RDS database schema.\n",
    "  \n",
    "We fetch Pandas dataframe object and we examine it.\n",
    "\n",
    "***N.B.:***We started scrapping the bikes data on 02/03/2020"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create dataframe and store data running SQL query\n",
    "df_AllStations = pd.read_sql_query(\"select *,cast(str_to_date(availability.datetime,'%d-%b-%Y (%H:%i:%s.%f)' )as datetime) as datetimeB FROM dublin.availability, dublin.Weather having abs(datetimeB-Weather.dateTime)<10800 and Weather.dateTime<'2020/3/29'\", con)\n",
    "\n",
    "# Examine dataframe object, show first 10 rows\n",
    "df_AllStations.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_AllStations.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#create column for day of the week\n",
    "df_AllStations['weekday'] = df_AllStations['datetimeB'].dt.dayofweek\n",
    "df_AllStations['hour'] = df_AllStations['datetimeB'].dt.hour\n",
    "df_AllStations['minutes'] = df_AllStations['datetimeB'].dt.minute"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#use number to replace weekdays\n",
    "df_AllStations['weekday'] = df_AllStations['weekday'].replace(0, 'Monday')\n",
    "df_AllStations['weekday'] = df_AllStations['weekday'].replace(1, 'Tuesday')\n",
    "df_AllStations['weekday'] = df_AllStations['weekday'].replace(2, 'Wednesday')\n",
    "df_AllStations['weekday'] = df_AllStations['weekday'].replace(3, 'Thursday')\n",
    "df_AllStations['weekday'] = df_AllStations['weekday'].replace(4, 'Friday')\n",
    "df_AllStations['weekday'] = df_AllStations['weekday'].replace(5, 'Saturday')\n",
    "df_AllStations['weekday'] = df_AllStations['weekday'].replace(6, 'Sunday')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#show all the columns of the dataframe\n",
    "df_AllStations.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dummy coding(categorical -> continuous)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We need to incorporate the following information (variables) in our model, but they are not immediately usable as they come in a categorical form:  \n",
    "**cloud coverage  \n",
    "day of the week**  \n",
    "Thus, we transform them in a series of dummy variables. The process is known as \"dummy coding\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a separate dataframe with days of the week (categorical)\n",
    "data_input1 = pd.DataFrame(df_AllStations['weekday'])\n",
    "\n",
    "# Create a separate dataframe with cloud coverage information (categorical)\n",
    "data_input2 = pd.DataFrame(df_AllStations['weatherMain'])\n",
    "\n",
    "# Concatenate the two dataframes in the main one\n",
    "dummy = pd.get_dummies(data_input1)\n",
    "dummy_2 = pd.get_dummies(data_input2)\n",
    "df_AllStations = pd.concat([df_AllStations,dummy],axis=1)\n",
    "df_AllStations = pd.concat([df_AllStations,dummy_2],axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Examine dataframe object, show first rows\n",
    "df_AllStations.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Check dataframe state"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### New columns\n",
    "The new dummy coded variables have been concatenated at the end of the dataframe, so we now check our dataframe shape."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show number of rows and columns of the dataframe\n",
    "print(\"The dataset has %s rows and %s columns.\" % df_AllStations.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Data types\n",
    "As a further check, we analyze the data type in our dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_AllStations.dtypes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ML model\n",
    "In order to predict our two target variables  \n",
    "\n",
    "**number of available bikes  \n",
    "number of available bike stands**  \n",
    "we need two differnt ML models that are trained in the following section.  \n",
    "After having tested the regression model as a viable alternative, we decided to implement a *Random Forest* classifier model, as it proves to be a more effective predictor.\n",
    "  \n",
    " ***N.B.:*** we train our models on a random selection of 2/3 of the original dataset. We perform testing on the remain"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### (1)Predict the number of available bikes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**model training**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select model features and store them in a new dataframe\n",
    "input_model = pd.DataFrame(df_AllStations[['number','temperature','windSpeed','hour','weekday_Friday', 'weekday_Monday',\n",
    "       'weekday_Saturday', 'weekday_Sunday', 'weekday_Thursday',\n",
    "       'weekday_Tuesday', 'weekday_Wednesday']])\n",
    "input_model = pd.concat([input_model,dummy_2],axis=1)\n",
    "\n",
    "# Define target variable\n",
    "output = df_AllStations['available_bikes']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split dataset to train and test\n",
    "X_train,X_test,Y_train,Y_test=train_test_split(input_model,output,test_size=0.33,random_state=42)\n",
    "print(\"Training the model on %s rows and %s columns.\" % X_train.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instantiate RandomForestRegressor object calling 100 decision tree models\n",
    "rf = RandomForestRegressor(n_estimators=100)\n",
    "\n",
    "# Train the model\n",
    "rf.fit(X_train, Y_train)\n",
    "\n",
    "print(\"Testing the model on %s rows.\" % Y_test.shape[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**model testing**  \n",
    "Using the trained model to predict the target feature availablebikes on the testing dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get prediction for test cases\n",
    "prediction = rf.predict(X_test)\n",
    "\n",
    "#make a new datafram to show the predicted available bikes\n",
    "DF_Predicated = pd.DataFrame(prediction, columns=['Predicted'])\n",
    "\n",
    "#convert all the data for testing to a new datafram\n",
    "DF_Alltest = df_AllStations.iloc[Y_test]\n",
    "\n",
    "#reset the index\n",
    "DF_Bikes = pd.DataFrame(DF_Alltest['available_bikes']).reset_index(drop=True)\n",
    "\n",
    "#to get a clear comparisaon, concatenate two new datafram\n",
    "actual_vs_predicted= pd.concat([DF_Bikes,DF_Predicated], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\nPredictions with multiple linear regression: \\n\")\n",
    "actual_vs_predicted"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Model evaluation**  \n",
    "In order to evaluate the prediction effectiveness of our model, we compute the mean-absolute error, the mean squared error,the root-mean-square deviation and R2."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def printMetrics(testActualVal, predictions):\n",
    "    #classification evaluation measures\n",
    "    print('Error Evaluation')\n",
    "    print('==============================================================================')\n",
    "    print(\"MAE (Mean Absolute Error): \", metrics.mean_absolute_error(testActualVal, predictions))\n",
    "    print(\"MSE (Mean Squared Error): \", metrics.mean_squared_error(testActualVal, predictions))\n",
    "    print(\"RMSE (Root Mean Squared Error): \", metrics.mean_squared_error(testActualVal, predictions)**0.5)\n",
    "    print(\"R2: \", metrics.r2_score(testActualVal, predictions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "printMetrics(Y_test, prediction)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The R² value implies that there is 94% less variation around the line than the mean. In other words, the relationship between the input variables and the number of available bikes accounts for 94% of the variation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Module integration**  \n",
    "In order to connect the ML model to our Flask web application, we need to produce a 'prediction-data' file from the trained model using the ***Pickle*** Python module.  \n",
    "Pickle allows us to store the prediction model in a file that we save on the server, in order to be used by the application to actually deliver a prediction based on the requested stations by the user."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pickle.dump(rf,open('final_prediction_bike.pickle', 'wb'))\n",
    "\n",
    "\n",
    "# This is not strictly functional to the application\n",
    "random_forest = pickle.load(open(\"final_prediction_bike.pickle\", \"rb\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### (2)Predict the number of available bikes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**model training**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select model features and store them in a new dataframe\n",
    "input_model = pd.DataFrame(df_AllStations[['number','temperature','windSpeed','hour','weekday_Friday', 'weekday_Monday',\n",
    "       'weekday_Saturday', 'weekday_Sunday', 'weekday_Thursday',\n",
    "       'weekday_Tuesday', 'weekday_Wednesday']])\n",
    "input_model = pd.concat([input_model,dummy_2],axis=1)\n",
    "\n",
    "# Define target variable\n",
    "output = df_AllStations['available_bike_stands']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split dataset to train and test\n",
    "X_train,X_test,Y_train,Y_test=train_test_split(input_model,output,test_size=0.33,random_state=42)\n",
    "print(\"Training the model on %s rows and %s columns.\" % X_train.shape)\n",
    "\n",
    "# Instantiate RandomForestRegressor object calling 100 decision tree models\n",
    "rf2 = RandomForestRegressor(n_estimators=100)\n",
    "\n",
    "# Train the model\n",
    "rf2.fit(X_train, Y_train)\n",
    "\n",
    "print(\"Testing the model on %s rows.\" % Y_test.shape[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**model testing**\n",
    "  \n",
    " Using the trained model to predict the target feature availablebikes on the testing dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get prediction for test cases\n",
    "prediction = rf2.predict(X_test)\n",
    "\n",
    "#make a new datafram to show the predicted available bikes\n",
    "DF_Predicated = pd.DataFrame(prediction, columns=['Predicted'])\n",
    "\n",
    "#convert all the data for testing to a new datafram\n",
    "DF_Alltest = df_AllStations.iloc[Y_test]\n",
    "\n",
    "#reset the index\n",
    "DF_Stands = pd.DataFrame(DF_Alltest['available_bike_stands']).reset_index(drop=True)\n",
    "\n",
    "#to get a clear comparisaon, concatenate two new datafram\n",
    "actual_vs_predicted= pd.concat([DF_Stands,DF_Predicated], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\nPredictions with multiple linear regression: \\n\")\n",
    "actual_vs_predicted"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**model evaluation**  \n",
    "In order to evaluate the prediction effectiveness of our model, we compute the mean-absolute error, the mean squared error,the root-mean-square deviation and R2."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def printMetrics(testActualVal, predictions):\n",
    "    #classification evaluation measures\n",
    "    print('Error Evaluation of stands')\n",
    "    print('==============================================================================')\n",
    "    print(\"MAE (Mean Absolute Error): \", metrics.mean_absolute_error(testActualVal, predictions))\n",
    "    print(\"MSE (Mean Squared Error): \", metrics.mean_squared_error(testActualVal, predictions))\n",
    "    print(\"RMSE (Root Mean Squared Error): \", metrics.mean_squared_error(testActualVal, predictions)**0.5)\n",
    "    print(\"R2: \", metrics.r2_score(testActualVal, predictions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "printMetrics(Y_test, prediction)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The R² value implies that there is 94% less variation around the line than the mean. In other words, the relationship between the input variables and the number of available bikes accounts for 94% of the variation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Module integration**  \n",
    "In order to connect the ML model to our Flask web application, we need to produce a 'prediction-data' file from the trained model using the ***Pickle*** Python module.  \n",
    "Pickle allows us to store the prediction model in a file that we save on the server, in order to be used by the application to actually deliver a prediction based on the requested stations by the user."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pickle.dump(rf2,open('final_prediction_bike_stands.pickle', 'wb'))\n",
    "\n",
    "# This is not strictly functional to the application\n",
    "random_forest_stands=pickle.load(open(\"final_prediction_bike_stands.pickle\", \"rb\"))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
